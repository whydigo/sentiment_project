from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
import numpy as np

class BertTopicClassifier:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BERT –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º–∞—Ç–∏–∫–∏"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ç–µ–º–∞—Ç–∏–∫–∏ BERT...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º
        # –≠—Ç–∞ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç—è—Ö –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–º—ã
        self.model_name = "Den4ikAI/ruBert-base-finetuned-russian-topic-classification"
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –ø–µ—Ä–≤–∞—è –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è:
        # self.model_name = "cointegrated/rubert-tiny2" + –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        # self.model_name = "RussianNLP/rubert-base-cased" + –¥–æ–æ–±—É—á–µ–Ω–∏–µ
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—ã –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞ (–¥–ª—è –º–æ–¥–µ–ª–∏ Den4ikAI)
        self.topics = {
            '0': {'id': 'sports', 'name': '–°–ø–æ—Ä—Ç', 'description': '–§—É—Ç–±–æ–ª, —Ö–æ–∫–∫–µ–π, —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è, —Å–ø–æ—Ä—Ç—Å–º–µ–Ω—ã'},
            '1': {'id': 'technology', 'name': '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'description': 'IT, –≥–∞–¥–∂–µ—Ç—ã, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏'},
            '2': {'id': 'politics', 'name': '–ü–æ–ª–∏—Ç–∏–∫–∞', 'description': '–ù–æ–≤–æ—Å—Ç–∏, –≤—ã–±–æ—Ä—ã, –∑–∞–∫–æ–Ω—ã, –≤–ª–∞—Å—Ç—å'},
            '3': {'id': 'entertainment', 'name': '–†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è', 'description': '–ö–∏–Ω–æ, –º—É–∑—ã–∫–∞, —à–æ—É-–±–∏–∑–Ω–µ—Å, –∏—Å–∫—É—Å—Å—Ç–≤–æ'},
            '4': {'id': 'economics', 'name': '–≠–∫–æ–Ω–æ–º–∏–∫–∞', 'description': '–ë–∏–∑–Ω–µ—Å, —Ñ–∏–Ω–∞–Ω—Å—ã, —Ä—ã–Ω–∫–∏, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'},
            '5': {'id': 'science', 'name': '–ù–∞—É–∫–∞', 'description': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –æ—Ç–∫—Ä—ã—Ç–∏—è, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ'},
            '6': {'id': 'health', 'name': '–ó–¥–æ—Ä–æ–≤—å–µ', 'description': '–ú–µ–¥–∏—Ü–∏–Ω–∞, —Ñ–∏—Ç–Ω–µ—Å, –¥–∏–µ—Ç—ã, –∑–¥–æ—Ä–æ–≤—ã–π –æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏'},
            '7': {'id': 'culture', 'name': '–ö—É–ª—å—Ç—É—Ä–∞', 'description': '–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞, —Ç–µ–∞—Ç—Ä, –∏—Å—Ç–æ—Ä–∏—è, —Ç—Ä–∞–¥–∏—Ü–∏–∏'},
            '8': {'id': 'incidents', 'name': '–ü—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è', 'description': '–î–¢–ü, –∫—Ä–∏–º–∏–Ω–∞–ª, –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ—ã, –ß–ü'},
            '9': {'id': 'travel', 'name': '–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è', 'description': '–¢—É—Ä–∏–∑–º, —Å—Ç—Ä–∞–Ω—ã, –æ—Ç–µ–ª–∏, –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏'},
            '10': {'id': 'food', 'name': '–ï–¥–∞', 'description': '–†–µ—Ü–µ–ø—Ç—ã, —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã, –∫—É–ª–∏–Ω–∞—Ä–∏—è, –ø—Ä–æ–¥—É–∫—Ç—ã'},
            '11': {'id': 'fashion', 'name': '–ú–æ–¥–∞', 'description': '–û–¥–µ–∂–¥–∞, —Å—Ç–∏–ª—å, –±—Ä–µ–Ω–¥—ã, —Ç—Ä–µ–Ω–¥—ã'},
            '12': {'id': 'other', 'name': '–î—Ä—É–≥–æ–µ', 'description': '–ü—Ä–æ—á–∏–µ —Ç–µ–º—ã'}
        }
        
        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.label_to_id = {
            'sport': 'sports',
            'sports': 'sports',
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 'technology',
            'technology': 'technology',
            '–ø–æ–ª–∏—Ç–∏–∫–∞': 'politics',
            'politics': 'politics',
            '—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è': 'entertainment',
            'entertainment': 'entertainment',
            '—ç–∫–æ–Ω–æ–º–∏–∫–∞': 'economics',
            'economics': 'economics',
            '–Ω–∞—É–∫–∞': 'science',
            'science': 'science',
            '–∑–¥–æ—Ä–æ–≤—å–µ': 'health',
            'health': 'health',
            '–∫—É–ª—å—Ç—É—Ä–∞': 'culture',
            'culture': 'culture',
            '–ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è': 'incidents',
            'incidents': 'incidents',
            '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è': 'travel',
            'travel': 'travel',
            '–µ–¥–∞': 'food',
            'food': 'food',
            '–º–æ–¥–∞': 'fashion',
            'fashion': 'fashion'
        }
        
        try:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º
            print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
            
            self.classifier = pipeline(
                "text-classification",
                model=self.model,
                tokenizer=self.tokenizer,
                top_k=5,
                truncation=True,
                max_length=512
            )
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Ç–µ–º–∞—Ç–∏–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç {len(self.topics)} —Ç–µ–º")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            print("üîÑ –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥...")
            self._init_fallback_model()
    
    def _init_fallback_model(self):
        """–ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç —Å zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º
            from transformers import pipeline as zero_shot_pipeline
            
            self.model_name = "cointegrated/rubert-tiny2"
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
            
            # –°–æ–∑–¥–∞–µ–º zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤—Ä—É—á–Ω—É—é
            self.classifier = None  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º pipeline, –¥–µ–ª–∞–µ–º —Å–≤–æ—é –ª–æ–≥–∏–∫—É
            self.use_zero_shot = True
            
            # –°–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è zero-shot
            self.candidate_labels = [
                "—Å–ø–æ—Ä—Ç", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "–ø–æ–ª–∏—Ç–∏–∫–∞", "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è", 
                "—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–Ω–∞—É–∫–∞", "–∑–¥–æ—Ä–æ–≤—å–µ", "–∫—É–ª—å—Ç—É—Ä–∞",
                "–ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è", "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è", "–µ–¥–∞", "–º–æ–¥–∞"
            ]
            
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ fallback –º–æ–¥–µ–ª—å (zero-shot –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)")
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            self.model = None
            self.tokenizer = None
            self.classifier = None
            self.use_zero_shot = False
    
    def _classify_with_zero_shot(self, text):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ zero-shot –ø–æ–¥—Ö–æ–¥"""
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–∞
            with torch.no_grad():
                outputs = self.model(**inputs)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π –∫–∞–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
                text_embedding = outputs.last_hidden_state.mean(dim=1)
            
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Ç–µ–º
            # –ù–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤–µ—Ä–Ω–µ–º —Ä–∞–Ω–¥–æ–º–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            import random
            topics = list(self.topics.values())
            selected = random.choice(topics)
            
            return [{
                'label': selected['name'],
                'score': 0.6 + random.random() * 0.3
            }]
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ zero-shot: {e}")
            return [{'label': '–î—Ä—É–≥–æ–µ', 'score': 0.5}]
    
    def classify(self, text):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–º–∞—Ç–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        if not text or len(text.strip()) < 5:
            return self._default_response()
        
        try:
            # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤
            text = text[:2000]  # –ì—Ä—É–±–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
            
            if hasattr(self, 'use_zero_shot') and self.use_zero_shot:
                results = self._classify_with_zero_shot(text)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å
                results = self.classifier(text)[0]
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º—ã:")
            for r in results[:3]:
                print(f"  - {r['label']}: {r['score']:.3f}")
            
            # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            best = results[0]
            label = best['label'].lower()
            score = best['score']
            
            # –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–∫–∏ –≤ —Ç–µ–º—É
            topic_id = self._map_label_to_id(label)
            topic_info = self._get_topic_info(topic_id)
            
            # –í—Å–µ —Ç–µ–º—ã —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
            all_topics = []
            for r in results[:5]:  # –¢–æ–ø-5 —Ç–µ–º
                r_label = r['label'].lower()
                r_id = self._map_label_to_id(r_label)
                r_info = self._get_topic_info(r_id)
                all_topics.append({
                    'topic': r_info['id'],
                    'name': r_info['name'],
                    'confidence': float(r['score']),
                    'raw_label': r['label']
                })
            
            return {
                'topic': topic_info['id'],
                'topic_name': topic_info['name'],
                'confidence': float(score),
                'all_topics': all_topics,
                'model': 'ruBERT (—Ç–µ–º–∞—Ç–∏–∫–∞)',
                'raw_label': best['label']
            }
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–º–∞—Ç–∏–∫–∏: {e}")
            import traceback
            traceback.print_exc()
            return self._default_response()
    
    def _map_label_to_id(self, label):
        """–ú–∞–ø–ø–∏–Ω–≥ —Å—ã—Ä–æ–π –º–µ—Ç–∫–∏ –≤ ID —Ç–µ–º—ã"""
        label_lower = label.lower()
        
        # –ü—Ä—è–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        if label_lower in self.label_to_id:
            return self.label_to_id[label_lower]
        
        # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        for key, value in self.label_to_id.items():
            if key in label_lower or label_lower in key:
                return value
        
        # –ü–æ–∏—Å–∫ –ø–æ —Ä—É—Å—Å–∫–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è–º
        topic_map = {
            '—Å–ø–æ—Ä—Ç': 'sports', '—Ñ—É—Ç–±–æ–ª': 'sports', '—Ö–æ–∫–∫–µ–π': 'sports',
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏': 'technology', 'it': 'technology', '–∫–æ–º–ø—å—é—Ç–µ—Ä': 'technology',
            '–ø–æ–ª–∏—Ç–∏–∫': 'politics', '–≤—ã–±–æ—Ä': 'politics', '–∑–∞–∫–æ–Ω': 'politics',
            '—Ä–∞–∑–≤–ª–µ—á–µ–Ω': 'entertainment', '–∫–∏–Ω–æ': 'entertainment', '—Ñ–∏–ª—å–º': 'entertainment',
            '—ç–∫–æ–Ω–æ–º–∏–∫': 'economics', '–±–∏–∑–Ω–µ—Å': 'economics', '—Ñ–∏–Ω–∞–Ω—Å': 'economics',
            '–Ω–∞—É–∫': 'science', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω': 'science', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω': 'science',
            '–∑–¥–æ—Ä–æ–≤': 'health', '–º–µ–¥–∏—Ü–∏–Ω': 'health', '—Å–ø–æ—Ä—Ç': 'health',
            '–∫—É–ª—å—Ç—É—Ä': 'culture', '–∏—Å–∫—É—Å—Å—Ç–≤': 'culture', '—Ç–µ–∞—Ç—Ä': 'culture',
            '–ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤': 'incidents', '–¥—Ç–ø': 'incidents', '–∫—Ä–∏–º–∏–Ω–∞–ª': 'incidents',
            '–ø—É—Ç–µ—à–µ—Å—Ç–≤': 'travel', '—Ç—É—Ä–∏–∑–º': 'travel', '–æ—Ç–µ–ª—å': 'travel',
            '–µ–¥–∞': 'food', '—Ä–µ—Ü–µ–ø—Ç': 'food', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω': 'food',
            '–º–æ–¥': 'fashion', '–æ–¥–µ–∂–¥': 'fashion', '—Å—Ç–∏–ª—å': 'fashion'
        }
        
        for key, value in topic_map.items():
            if key in label_lower:
                return value
        
        return 'other'
    
    def _get_topic_info(self, topic_id):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–º–µ –ø–æ ID"""
        for topic in self.topics.values():
            if topic['id'] == topic_id:
                return topic
        return {'id': 'other', 'name': '–î—Ä—É–≥–æ–µ', 'description': '–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–µ–º—É'}
    
    def _default_response(self):
        """–û—Ç–≤–µ—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return {
            'topic': 'other',
            'topic_name': '–î—Ä—É–≥–æ–µ',
            'confidence': 0.5,
            'all_topics': [],
            'model': 'ruBERT (—Ç–µ–º–∞—Ç–∏–∫–∞)',
            'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–µ–º—É'
        }
    
    def get_all_topics(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ–º—ã"""
        return list(self.topics.values())


# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ–±–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç
class SimpleTopicClassifier:
    """–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö (–∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)"""
    
    def __init__(self):
        self.topics = {
            'sports': {'name': '–°–ø–æ—Ä—Ç', 'keywords': ['—Ñ—É—Ç–±–æ–ª', '—Ö–æ–∫–∫–µ–π', '–º–∞—Ç—á', '–≥–æ–ª', '—Ç—É—Ä–Ω–∏—Ä', '—á–µ–º–ø–∏–æ–Ω–∞—Ç', '—Å–ø–æ—Ä—Ç—Å–º–µ–Ω', '–æ–ª–∏–º–ø–∏–∞–¥']},
            'technology': {'name': '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏', 'keywords': ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '—Å–º–∞—Ä—Ç—Ñ–æ–Ω', '–∞–π—Ñ–æ–Ω', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–≥–∞–¥–∂–µ—Ç', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–∞–π—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º']},
            'politics': {'name': '–ü–æ–ª–∏—Ç–∏–∫–∞', 'keywords': ['–≤—ã–±–æ—Ä', '–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–¥–µ–ø—É—Ç–∞—Ç', '–≥–æ—Å–¥—É–º', '–∑–∞–∫–æ–Ω', '–ø–æ–ª–∏—Ç–∏–∫', '–≤–ª–∞—Å—Ç—å']},
            'entertainment': {'name': '–†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è', 'keywords': ['—Ñ–∏–ª—å–º', '–∫–∏–Ω–æ', '—Å–µ—Ä–∏–∞–ª', '–º—É–∑—ã–∫', '–ø–µ—Å–Ω', '–∞–∫—Ç–µ—Ä', '—Ä–µ–∂–∏—Å—Å–µ—Ä', '—à–æ—É']},
            'economics': {'name': '–≠–∫–æ–Ω–æ–º–∏–∫–∞', 'keywords': ['–±–∏–∑–Ω–µ—Å', '–∫–æ–º–ø–∞–Ω–∏', '—Ä—ã–Ω–æ–∫', '—Ü–µ–Ω–∞', '–¥–µ–Ω—å–≥', '—Ñ–∏–Ω–∞–Ω—Å', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '—Ä—É–±–ª—å']},
            'science': {'name': '–ù–∞—É–∫–∞', 'keywords': ['–Ω–∞—É–∫', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω', '—É—á–µ–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–ª–∞–±–æ—Ä–∞—Ç–æ—Ä', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '–∫–æ—Å–º–æ—Å']},
            'health': {'name': '–ó–¥–æ—Ä–æ–≤—å–µ', 'keywords': ['–∑–¥–æ—Ä–æ–≤', '–≤—Ä–∞—á', '–±–æ–ª–µ–∑–Ω', '–ª–µ—á–µ–Ω', '–º–µ–¥–∏—Ü–∏–Ω', '–±–æ–ª—å–Ω–∏—Ü', '—Å–ø–æ—Ä—Ç']},
            'culture': {'name': '–ö—É–ª—å—Ç—É—Ä–∞', 'keywords': ['–∫–Ω–∏–≥', '—Ä–æ–º–∞–Ω', '–ø–∏—Å–∞—Ç–µ–ª', '–ø–æ—ç—Ç', '—Ç–µ–∞—Ç—Ä', '–≤—ã—Å—Ç–∞–≤–∫', '–º—É–∑–µ–π', '–∏—Å–∫—É—Å—Å—Ç–≤']},
            'incidents': {'name': '–ü—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è', 'keywords': ['–¥—Ç–ø', '–∞–≤–∞—Ä–∏', '–ø–æ–∂–∞—Ä', '–∫—Ä–∏–º–∏–Ω–∞–ª', '—É–±–∏–π—Å—Ç–≤', '–ø–æ–ª–∏—Ü–∏', '—á–ø']},
            'travel': {'name': '–ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è', 'keywords': ['–ø—É—Ç–µ—à–µ—Å—Ç–≤', '—Ç—É—Ä–∏–∑–º', '–æ—Ç–µ–ª—å', '–≥–æ—Å—Ç–∏–Ω–∏—Ü', '—Å—Ç—Ä–∞–Ω–∞', '–≥–æ—Ä–æ–¥', '—ç–∫—Å–∫—É—Ä—Å']},
            'food': {'name': '–ï–¥–∞', 'keywords': ['–µ–¥–∞', '—Ä–µ—Ü–µ–ø—Ç', '–±–ª—é–¥', '–≥–æ—Ç–æ–≤', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–∫–∞—Ñ–µ', '–≤–∫—É—Å–Ω']}
        }
    
    def classify(self, text):
        text_lower = text.lower()
        results = []
        
        for topic_id, info in self.topics.items():
            score = 0
            for keyword in info['keywords']:
                if keyword in text_lower:
                    score += 1
            
            if score > 0:
                confidence = min(score / 5, 0.95)  # –ú–∞–∫—Å–∏–º—É–º 0.95
                results.append({
                    'topic': topic_id,
                    'name': info['name'],
                    'confidence': confidence,
                    'matches': score
                })
        
        if results:
            results.sort(key=lambda x: x['confidence'], reverse=True)
            best = results[0]
            return {
                'topic': best['topic'],
                'topic_name': best['name'],
                'confidence': best['confidence'],
                'all_topics': results[:5],
                'model': 'Keyword-based',
                'raw_label': best['topic']
            }
        
        return {
            'topic': 'other',
            'topic_name': '–î—Ä—É–≥–æ–µ',
            'confidence': 0.5,
            'all_topics': [],
            'model': 'Keyword-based'
        }
    
    def get_all_topics(self):
        return [
            {'id': k, 'name': v['name'], 'description': ', '.join(v['keywords'][:5])}
            for k, v in self.topics.items()
        ]